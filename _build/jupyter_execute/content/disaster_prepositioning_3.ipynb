{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Production Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting our code snippets into production-level code, we have a few goals,\n",
    "1. **Readability**:\tOur code should be easy to understand and follow, even for someone who didn’t write it. Clear naming conventions, well-structured functions, and comments where necessary help ensure that others can read and maintain the code effectively.\n",
    "\n",
    "\n",
    "2.\t**Robustness**: The code needs to handle different edge cases and potential errors gracefully, without breaking. This means writing code that checks for invalid inputs, unexpected conditions, and includes error handling to keep the program running smoothly.\n",
    "\n",
    "3.\t**Testing**: Code should be tested thoroughly to ensure it works as expected in various scenarios. This involves writing test cases that cover different functionalities, both common and rare, to catch bugs early and maintain confidence in the code’s reliability.\n",
    "\n",
    "To achieve these goals, we need a solid design approach that provides structure and flexibility to our code. This is where Object-Oriented Programming (OOP) comes in. It's one of the most popular frameworks, and you've likely worked with it in class or otherwise. OOP offers a way to organize code into logical units that make it easier to read, robust, and testable. By structuring our code around objects that encapsulate data and functionality, we create a modular framework that simplifies complex data operations and supports future growth.\n",
    "\n",
    "OOP promotes modularity, reusability, and maintainability, allowing for a clearer separation of concerns and reducing the likelihood of errors. Additionally, OOP’s use of abstraction and inheritance helps create reusable components, ensuring consistent and efficient code across all parts of the project. So, what does this all actually look like in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a general flow diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flowchart' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflowchart\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flowchart' is not defined"
     ]
    }
   ],
   "source": [
    "flowchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reader and Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage in the process is obviously cleaning and processing the data, and one of the interesting things that you may not have seen in course-work is the segmentation of the data reader class and the dataset class. Often in smaller projects, the reader functionality is added as a class method, many times within the instantiation call, which can make intuitive sense. However, there are a few key benefits to separating them out when we work with larger projects.\n",
    "\n",
    "1. Separation of Concerns: By creating separate classes, each class is responsible for a distinct function — the data reader class handles the loading and preprocessing of data from various sources, while the dataset class manages how the data is stored, accessed, and manipulated. This separation makes the codebase easier to understand and modify, as changes to how data is read do not impact how the data is organized or utilized downstream.\n",
    "\n",
    "\n",
    "2.\tReusability: When data reading and data management are isolated into their own classes, they can be reused across different projects or tasks without modification. For instance, the same data reader class can be employed to load data from multiple sources (like CSVs, databases, or APIs) while reusing the dataset class to provide a uniform interface for accessing and manipulating data, regardless of its origin.\n",
    "3.\tMaintainability: Segmenting these responsibilities simplifies debugging and testing. If there is an issue with data loading, the problem is likely within the data reader class, whereas issues with data handling can be traced back to the dataset class. This modularity reduces the risk of unintended side effects when making changes or improvements, since each class operates independently.\n",
    "4.\tFlexibility: A segmented approach allows developers to quickly adapt to changes in data sources or formats without needing to rewrite core data handling logic. The data reader class can be updated to handle new data formats or sources, while the dataset class remains consistent in providing a structured interface for data access and manipulation.\n",
    "5.  Testability: Perhaps one of the largest advantages is a layout that more easily lends itself to testing. If someone manually creates a test set, either class can be tested regardless of the other's state, and where the errors lie is often much easier to figure out\n",
    "\n",
    "As you'll come to see, one of the core principles used to develop large projects like this is similar to the idea of a black box from machine learning. With multiple people working together, there is an emphasis on smoothly integrating everyone's work. So this approach can allow a potentially difficult task (loading and preparing the data) to be distributed among developers without requiring everyone to understand all the intricacies of each class\n",
    "\n",
    "Let's see how this has been implemented!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Country:\n",
    "    id: str = field(hash=True)\n",
    "    continent: str = field(repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Location:\n",
    "    id: str = field(hash=True)\n",
    "    address: str = field(repr=False)\n",
    "    country: Country = field(repr=False)\n",
    "    latitude: float = field(repr=False)\n",
    "    longitude: float = field(repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterType:\n",
    "    id: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterImpact:\n",
    "    id: str\n",
    "    disaster: \"Disaster\" = field(repr=False)\n",
    "    location: \"DisasterLocation\" = field(repr=False)\n",
    "    sub_location_nr: int = field(repr=False)\n",
    "    total_affected: int = field(repr=False)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.id\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DisasterLocation(Location):\n",
    "    def __repr__(self):\n",
    "        return self.id\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Disaster:\n",
    "    id: str\n",
    "    type: DisasterType = field(repr=False)\n",
    "    day: int = field(repr=False)\n",
    "    month: int = field(repr=False)\n",
    "    year: int = field(repr=False)\n",
    "    impacted_locations: list[DisasterImpact] = field(hash=False, repr=False)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Depot(Location):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Item:\n",
    "    id: str = field(hash=True)\n",
    "    weight: float = field(repr=False)  # Metric tons\n",
    "    volume: float = field(repr=False)  # Cubic metres\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransportMode:\n",
    "    id: str = field(hash=True)\n",
    "    distance_method: str\n",
    "    big_m_cost_elim: float\n",
    "    max_driving_time_cut_above_hrs: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DistanceInfo:\n",
    "    distance: float  # Kilometres\n",
    "    time: float  # Hours\n",
    "    cost_per_ton: float  # USD\n",
    "\n",
    "DistanceMatrix = dict[Tuple[Location, Location, TransportMode], DistanceInfo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class encapsulates all data required for the analysis and optimization tasks. It includes lists of depots, disasters, and disaster_locations, and dictionaries for probabilities of disasters, inventory levels, and other critical data. The _zero_demand_threshold is a constant used internally to determine when demand should be considered negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Dataset:\n",
    "    depots: list[Depot]\n",
    "    disasters: list[Disaster]\n",
    "    disaster_locations: list[DisasterLocation]\n",
    "    probabilities: dict[Disaster, float]\n",
    "    items: list[Item]\n",
    "    transport_modes: list[TransportMode]\n",
    "    inventory: dict[Tuple[Depot, Item], int]\n",
    "    inventory_scenarios: dict[str, dict[Tuple[Depot, Item], int]]\n",
    "    distance: DistanceMatrix\n",
    "    people_affected: dict[Tuple[DisasterImpact, Item], float]\n",
    "    persons_per_item_general: dict[Tuple[DisasterImpact, Item], float]\n",
    "    persons_per_item_monthly: dict[Tuple[DisasterImpact, Item], float]\n",
    "    disaster_affected_totals: dict[str, int]\n",
    "\n",
    "    _zero_demand_threshold = 1e6\n",
    "\n",
    "\n",
    "    '''\n",
    "    The take_disaster_subset method allows us to focus our analysis on a specific \n",
    "    subset of disasters by providing a predicate function that filters disasters \n",
    "    based on certain criteria (e.g., date range, disaster type). It recalculates \n",
    "    probabilities to ensure they sum up to 1 after filtering. Related data such as \n",
    "    locations, distances, and affected populations are also filtered to maintain \n",
    "    consistency. If no filtering occurs (i.e., all disasters are included), the method \n",
    "    returns the original dataset to avoid unnecessary duplication.\n",
    "    '''\n",
    "\n",
    "    def take_disaster_subset(self, predicate: Callable[[Disaster], bool]) -> \"Dataset\":\n",
    "        \"\"\"\n",
    "        Generate a smaller dataset by only selecting a subset of the disasters with corresponding data\n",
    "        \"\"\"\n",
    "        disasters = list(filter(predicate, self.disasters))\n",
    "\n",
    "        if len(disasters) == len(self.disasters):\n",
    "            return self\n",
    "\n",
    "        total_probability = sum(self.probabilities[disaster] for disaster in disasters)\n",
    "        probabilities = {\n",
    "            disaster: self.probabilities[disaster] / total_probability\n",
    "            for disaster in disasters\n",
    "        }\n",
    "        locations = [\n",
    "            impact.location\n",
    "            for disaster in disasters\n",
    "            for impact in disaster.impacted_locations\n",
    "        ]\n",
    "        distance = {\n",
    "            (source, destination, mode): cell\n",
    "            for (source, destination, mode), cell in self.distance.items()\n",
    "            if destination in locations\n",
    "        }\n",
    "        people_affected = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.people_affected.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        persons_per_item_general = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.persons_per_item_general.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        persons_per_item_monthly = {\n",
    "            (location, item): value\n",
    "            for (location, item), value in self.persons_per_item_monthly.items()\n",
    "            if location in locations\n",
    "        }\n",
    "        return Dataset(\n",
    "            self.depots,\n",
    "            disasters,\n",
    "            locations,\n",
    "            probabilities,\n",
    "            self.items,\n",
    "            self.transport_modes,\n",
    "            self.inventory,\n",
    "            self.inventory_scenarios,\n",
    "            distance,\n",
    "            people_affected,\n",
    "            persons_per_item_general,\n",
    "            persons_per_item_monthly,\n",
    "            self.disaster_affected_totals,\n",
    "        )\n",
    "    \n",
    "    '''\n",
    "    The take_inventory_scenario method enables us to switch between different \n",
    "    inventory configurations stored in inventory_scenarios. By specifying a filename \n",
    "    key, we retrieve the corresponding inventory data. If the requested inventory \n",
    "    is already in use, the method returns the current dataset. Otherwise, it creates \n",
    "    a new Dataset instance with the updated inventory, facilitating comparative \n",
    "    analysis of different supply scenarios.\n",
    "    '''\n",
    "\n",
    "    def take_inventory_scenario(self, filename: str):\n",
    "        if filename not in self.inventory_scenarios:\n",
    "            raise RuntimeError(\"Inventory scenario not found\")\n",
    "        inventory = self.inventory_scenarios[filename]\n",
    "        if inventory == self.inventory:\n",
    "            return self\n",
    "        return Dataset(\n",
    "            self.depots,\n",
    "            self.disasters,\n",
    "            self.disaster_locations,\n",
    "            self.probabilities,\n",
    "            self.items,\n",
    "            self.transport_modes,\n",
    "            inventory,\n",
    "            self.inventory_scenarios,\n",
    "            self.distance,\n",
    "            self.people_affected,\n",
    "            self.persons_per_item_general,\n",
    "            self.persons_per_item_monthly,\n",
    "            self.disaster_affected_totals,\n",
    "        )\n",
    "    '''\n",
    "    The general_demand property calculates the overall demand for each \n",
    "    item at each impacted location using general beta values \n",
    "    (persons_per_item_general). The beta value represents how many people \n",
    "    can be served per item. We utilize the _calc_items_needed helper method \n",
    "    for calculations. The @functools.cached_property decorator ensures that \n",
    "    the result is computed once and cached, improving performance for repeated \n",
    "    access.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    @functools.cached_property\n",
    "    def general_demand(self) -> dict[Tuple[DisasterImpact, Item], float]:\n",
    "        general_demand = {\n",
    "            (location, item): self._calc_items_needed(\n",
    "                self.people_affected[location, item],\n",
    "                self.persons_per_item_general[location, item],\n",
    "            )\n",
    "            for disaster in self.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "            for item in self.items\n",
    "        }\n",
    "\n",
    "        return {key: value for key, value in general_demand.items() if value > 1e-1}\n",
    "\n",
    "\n",
    "    '''\n",
    "    Similarly, the monthly_demand property calculates demand on a monthly \n",
    "    basis using persons_per_item_monthly. This allows us to capture time-sensitive \n",
    "    demand variations, which are crucial in disaster response. The result is \n",
    "    cached, and we filter out values below 1e-3 to maintain computational efficiency.\n",
    "    '''\n",
    "\n",
    "    @functools.cached_property\n",
    "    def monthly_demand(self) -> dict[Tuple[DisasterImpact, Item], float]:\n",
    "        monthly_demand = {\n",
    "            (location, item): self._calc_items_needed(\n",
    "                self.people_affected[location, item],\n",
    "                self.persons_per_item_monthly[location, item],\n",
    "            )\n",
    "            for disaster in self.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "            for item in self.items\n",
    "        }\n",
    "\n",
    "        return {key: value for key, value in monthly_demand.items() if value > 1e-3}\n",
    "\n",
    "    '''\n",
    "    The _calc_items_needed helper method computes the number of items required \n",
    "    based on the number of people affected and the beta value. If beta is zero or \n",
    "    exceeds a predefined threshold (_zero_demand_threshold), indicating negligible \n",
    "    demand, the method returns zero. Otherwise, it divides people_affected by beta \n",
    "    to determine the required items. This calculation is fundamental for planning supply \n",
    "    quantities in logistics.\n",
    "    '''\n",
    "    \n",
    "    def _calc_items_needed(\n",
    "        self,\n",
    "        people_affected: float,\n",
    "        beta: float,\n",
    "    ):\n",
    "        if beta == 0 or beta >= self._zero_demand_threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return people_affected / beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from section 2.6, to solve for optimal allocations we have to consider several independent items, which requires solving the model for each version. This can get messy pretty quickly considering the complexity of the model and the dataset that it uses. So in the actual implementation, the developers created a class to handle this larger loop. It will set up the problem for a specific context, and then call a dedicated solver class. \n",
    "\n",
    "While we won't go into it here, this set up also lends itself nicely to leveraging multithreading. You'll notice that there is a sub-class in the code which creates \"workers\". These allow use to handle each problem independently and is a common set-up in python for multi-threading. Luckily for our sakes, the problem overall is fast enough to be solved on a single instance, so this hasn't been implemented fully in production code!\n",
    "\n",
    "So let's take a look at how it all comes together.\n",
    "\n",
    "Firsr we'll define our name space and methods used from the data reader class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from multiprocessing import Pool\n",
    "from os import getenv\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.data import Dataset, Disaster, DisasterImpact, DistanceInfo, Item\n",
    "from src.solving import (\n",
    "    AllocationStrategy,\n",
    "    CostMatrix,\n",
    "    Problem,\n",
    "    Solution,\n",
    "    SolverParameters,\n",
    "    StochasticSolver,\n",
    ")\n",
    "\n",
    "class SolverObjective(IntEnum):\n",
    "    Cost = (0,)\n",
    "    Time = (1,)\n",
    "    Distance = 2\n",
    "\n",
    "SolutionTags = tuple[SolverObjective, AllocationStrategy]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get into the meat of it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, AnalysisParameters is a configuration class that holds various parameters influencing the analysis process. These settings allow us to control aspects like which disasters to consider, the objectives to optimize for, and how to handle inventory and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisParameters:\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    expand_depot_set\n",
    "        Flag indicating whether inventory can be reallocated to depots that don't currently hold any stock\n",
    "    care_about_month_demand\n",
    "        Flag indicating whether we take month-by-month demand (True) or the general number (False)\n",
    "    disaster_month\n",
    "        Month from which to select disasters\n",
    "    num_months_to_average\n",
    "        Number of months to use for selecting disasters, when disasterMonth>=0\n",
    "    optimization_objectives\n",
    "        Set of objectives to use for running the optimization model\n",
    "    comparison_objectives\n",
    "        Set of objectives to use for comparing results\n",
    "    allocation_strategies\n",
    "        Which strategies to test for (re)allocation inventory to depots in the first stage\n",
    "    min_year\n",
    "        First year from which disasters should be taken into account\n",
    "    max_year\n",
    "        Last year from which disasters should be taken into account\n",
    "    scale_demand\n",
    "        Whether demand must be scaled to not exceed total available inventory or not\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    expand_depot_set: bool = False\n",
    "    care_about_month_demand: bool = True\n",
    "    disaster_month: int = -1\n",
    "    num_months_to_average: int = 3\n",
    "    optimization_objectives: list[SolverObjective] = [\n",
    "        SolverObjective.Cost,\n",
    "        SolverObjective.Time,\n",
    "    ]\n",
    "    comparison_objectives: list[SolverObjective] = list(SolverObjective)\n",
    "    allocation_strategies: list[AllocationStrategy] = list(AllocationStrategy)\n",
    "    min_year: int = 1900\n",
    "    max_year: int = 2100\n",
    "    scale_demand: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analysis class encapsulates all results and statistics from the optimization runs. It includes solutions for different objectives and strategies, along with computed metrics that help in evaluating and comparing these solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    \"\"\"\n",
    "    Analysis results for multiple optimization runs for a single dataset and item, using different objectives and allocation strategies.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    parameters:\n",
    "        Parameters used to construct the analysis\n",
    "    dataset:\n",
    "        Original dataset being analyzed\n",
    "    item:\n",
    "        Item for which the analysis was performed\n",
    "    solutions:\n",
    "        Dictionary of solutions for all solved problems\n",
    "    solution_stats\n",
    "        Index: objective, strategy\n",
    "        Columns:\n",
    "        - coveredDemandExcDummy\n",
    "        - dualTotInv\n",
    "        - totalCostIncDummy\n",
    "        - totalCostExcDummy\n",
    "        - totalDemand\n",
    "        - fractionOfDisastersUsingDummy\n",
    "        - averageUnitCost\n",
    "        - demandFulfillmentFraction\n",
    "    balance_metric\n",
    "        Index: objective\n",
    "        Columns: balanceMetric\n",
    "    units_shipped\n",
    "        Index: objective, strategy, mode\n",
    "        Columns: unitsShipped, unitsShippedWeighted\n",
    "    people_served_per_item\n",
    "        Index: objective, strategy\n",
    "        Columns: peopleServedPerItem\n",
    "    cross_ompact\n",
    "        Index: objective, strategy, other\n",
    "        Columns: impact\n",
    "    \"\"\"\n",
    "\n",
    "    parameters: AnalysisParameters\n",
    "    dataset: Dataset\n",
    "    item: Item\n",
    "    solutions: dict[SolutionTags, Solution]\n",
    "    solution_stats: pd.DataFrame\n",
    "    balance_metric: pd.DataFrame\n",
    "    units_shipped: pd.DataFrame\n",
    "    people_served_per_item: pd.DataFrame\n",
    "    cross_impact: pd.DataFrame\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameters: AnalysisParameters,\n",
    "        dataset: Dataset,\n",
    "        item: Item,\n",
    "        solutions: dict[SolutionTags, Solution],\n",
    "        solution_stats: pd.DataFrame,\n",
    "        balance_metric: pd.DataFrame,\n",
    "        units_shipped: pd.DataFrame,\n",
    "        people_served_per_item: pd.DataFrame,\n",
    "        cross_impact: pd.DataFrame,\n",
    "    ):\n",
    "        self.parameters = parameters\n",
    "        self.dataset = dataset\n",
    "        self.item = item\n",
    "        self.solutions = solutions\n",
    "        self.solution_stats = solution_stats\n",
    "        self.balance_metric = balance_metric\n",
    "        self.units_shipped = units_shipped\n",
    "        self.people_served_per_item = people_served_per_item\n",
    "        self.cross_impact = cross_impact\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AnalyzerWorker class performs the core computation. It runs optimization models for different objectives and strategies, processes the results, and computes various metrics for analysis. The _post_process method compiles these results into meaningful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzerWorker:\n",
    "    def __init__(self, parameters: AnalysisParameters):\n",
    "        self.parameters = parameters\n",
    "        self._solver = StochasticSolver()\n",
    "\n",
    "    def run(self, dataset: Dataset, item: Item) -> Analysis:\n",
    "        #a=len(str(dataset))\n",
    "        dataset = self._filter_dataset(dataset)\n",
    "        #print(len(str(dataset))!=a)\n",
    "        probabilities = {\n",
    "            disaster: 1 / len(dataset.disasters) for disaster in dataset.disasters\n",
    "        }\n",
    "\n",
    "        solutions: dict[SolutionTags, Solution] = {}\n",
    "\n",
    "        # Construct cost matrices once\n",
    "        cost_matrices = {\n",
    "            objective: self._get_cost_matrix(dataset, item, objective)\n",
    "            for objective in SolverObjective\n",
    "        }\n",
    "\n",
    "        # Construct inventory\n",
    "        inventory = self._select_inventory(dataset, item)\n",
    "        if sum(inventory.values()) == 0:\n",
    "            return None\n",
    "\n",
    "        # Construct demand\n",
    "        demand = self._select_demand(dataset, item)\n",
    "\n",
    "        # Solve models for all objectives and strategies\n",
    "        for objective in self.parameters.optimization_objectives:\n",
    "            for strategy in self.parameters.allocation_strategies:\n",
    "                problem = Problem(\n",
    "                    dataset.depots,\n",
    "                    inventory,\n",
    "                    demand,\n",
    "                    dataset.disasters,\n",
    "                    probabilities,\n",
    "                    dataset.transport_modes,\n",
    "                    cost_matrices[objective],\n",
    "                )\n",
    "                parameters = SolverParameters(strategy, self.parameters.scale_demand)\n",
    "                tags = (objective, strategy)\n",
    "                solutions[tags] = self._solver.solve(problem, parameters)\n",
    "\n",
    "        return self._post_process(dataset, item, cost_matrices, solutions)\n",
    "\n",
    "    def dispose(self):\n",
    "        self._solver.dispose()\n",
    "\n",
    "    def _select_inventory(self, dataset: Dataset, item: Item):\n",
    "        return {\n",
    "            depot: dataset.inventory.get((depot, item), 0)\n",
    "            for depot in dataset.depots\n",
    "            if self.parameters.expand_depot_set\n",
    "            or dataset.inventory.get((depot, item), 0) > 0\n",
    "        }\n",
    "\n",
    "    def _filter_dataset(self, dataset: Dataset) -> Dataset:\n",
    "        if self.parameters.disaster_month > -1:\n",
    "            months = range(\n",
    "                self.parameters.disaster_month,\n",
    "                self.parameters.disaster_month\n",
    "                + 1\n",
    "                + self.parameters.num_months_to_average,\n",
    "            )\n",
    "            months = [(month - 1) % 12 + 1 for month in months]\n",
    "            predicate: Callable[[Disaster], bool] = (\n",
    "                lambda disaster: disaster.month in months\n",
    "            )\n",
    "            dataset = dataset.take_disaster_subset(predicate)\n",
    "\n",
    "        dataset = dataset.take_disaster_subset(\n",
    "            lambda disaster: disaster.year >= self.parameters.min_year\n",
    "            and disaster.year <= self.parameters.max_year\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _select_demand(\n",
    "        self, dataset: Dataset, item: Item\n",
    "    ) -> dict[DisasterImpact, float]:\n",
    "        source = (\n",
    "            dataset.monthly_demand\n",
    "            if self.parameters.care_about_month_demand\n",
    "            else dataset.general_demand\n",
    "        )\n",
    "        return {\n",
    "            location: source.get((location, item), 0)\n",
    "            for disaster in dataset.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "        }\n",
    "\n",
    "    def _get_cost_matrix(\n",
    "        self, dataset: Dataset, item: Item, objective: SolverObjective\n",
    "    ) -> CostMatrix:\n",
    "        return {\n",
    "            key: self._get_cost_element(value, objective, item)\n",
    "            for key, value in dataset.distance.items()\n",
    "        }\n",
    "\n",
    "    def _get_cost_element(\n",
    "        self, cell: DistanceInfo, objective: SolverObjective, item: Item\n",
    "    ):\n",
    "        if objective == SolverObjective.Cost:\n",
    "            return item.weight * cell.cost_per_ton\n",
    "        elif objective == SolverObjective.Time:\n",
    "            return cell.time\n",
    "        elif objective == SolverObjective.Distance:\n",
    "            return cell.distance\n",
    "        else:\n",
    "            raise RuntimeError(f\"Undefined objective {objective}\")\n",
    "\n",
    "    def _post_process(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        item: Item,\n",
    "        costs: dict[SolverObjective, CostMatrix],\n",
    "        solutions: dict[SolutionTags, Solution],\n",
    "    ):\n",
    "        beta_source = (\n",
    "            dataset.persons_per_item_monthly\n",
    "            if self.parameters.care_about_month_demand\n",
    "            else dataset.persons_per_item_general\n",
    "        )\n",
    "        beta = {\n",
    "            location.id: beta_source[location, item]\n",
    "            for disaster in dataset.disasters\n",
    "            for location in disaster.impacted_locations\n",
    "        }\n",
    "\n",
    "        solution_stats = pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"objective\": objective,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"coveredDemandExcDummy\": solution.covered_demand_exc_dummy,\n",
    "                    \"dualTotInv\": solution.dual_total_inventory,\n",
    "                    \"totalCostIncDummy\": solution.total_cost_inc_dummy,\n",
    "                    \"totalCostExcDummy\": solution.total_cost_exc_dummy,\n",
    "                    \"totalDemand\": solution.total_demand,\n",
    "                    \"fractionOfDisastersUsingDummy\": solution.fraction_of_disasters_using_dummy,\n",
    "                }\n",
    "                for (objective, strategy), solution in solutions.items()\n",
    "            ]\n",
    "        ).set_index([\"objective\", \"strategy\"])\n",
    "\n",
    "        df_flows = pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"objective\": objective,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"disaster\": disaster.id,\n",
    "                    \"depot\": depot.id,\n",
    "                    \"impact\": impact.id,\n",
    "                    \"location\": impact.location.id,\n",
    "                    \"mode\": mode.id,\n",
    "                    \"flow\": value,\n",
    "                    \"distance\": dataset.distance[depot, impact.location, mode].distance\n",
    "                    if depot.id != \"DUMMY\"\n",
    "                    else None,\n",
    "                }\n",
    "                for (objective, strategy), solution in solutions.items()\n",
    "                for (disaster, depot, impact, mode), value in solution.flow.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Average unit cost\n",
    "        solution_stats[\"averageUnitCost\"] = solution_stats[\"totalCostExcDummy\"] / (\n",
    "            solution_stats[\"coveredDemandExcDummy\"] + 1e-7\n",
    "        )\n",
    "\n",
    "        # Demand fulfillment fraction\n",
    "        solution_stats[\"demandFulfillmentFraction\"] = solution_stats[\n",
    "            \"coveredDemandExcDummy\"\n",
    "        ] / (solution_stats[\"totalDemand\"] + 1e-7)\n",
    "\n",
    "        # Balance metric\n",
    "        strategies = set(solution_stats.reset_index()[\"strategy\"])\n",
    "        pivoted = solution_stats.reset_index().pivot(\n",
    "            index=\"objective\", columns=\"strategy\", values=\"totalCostExcDummy\"\n",
    "        )\n",
    "        pivoted[\"balanceMetric\"] = (\n",
    "            pivoted[AllocationStrategy.MinimizeFixedInventory]\n",
    "            / (pivoted[AllocationStrategy.MinimizeTwoStage] + 1e-7)\n",
    "            if AllocationStrategy.MinimizeFixedInventory in strategies\n",
    "            and AllocationStrategy.MinimizeTwoStage in strategies\n",
    "            else None\n",
    "        )\n",
    "        balance_metric = pivoted\n",
    "\n",
    "        df_probabilities = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                disaster.id: dataset.probabilities[disaster]\n",
    "                for disaster in dataset.disasters\n",
    "            },\n",
    "            columns=[\"probability\"],\n",
    "            orient=\"index\",\n",
    "        )\n",
    "\n",
    "        # Units shipped\n",
    "        df_flow_no_dummy = df_flows.join(df_probabilities, on=\"disaster\")\n",
    "        df_flow_no_dummy = df_flow_no_dummy[\n",
    "            df_flow_no_dummy[\"depot\"] != \"DUMMY\"\n",
    "        ]  # TODO Replace hardcoded dummy ID\n",
    "        temp = df_flow_no_dummy.copy()\n",
    "        temp[\"unitsShipped\"] = temp[\"probability\"] * temp[\"flow\"]\n",
    "        temp[\"unitsShippedWeighted\"] = temp[\"unitsShipped\"] * temp[\"distance\"]\n",
    "        units_shipped = (\n",
    "            temp.set_index([\"objective\", \"strategy\", \"mode\"])[\n",
    "                [\"unitsShipped\", \"unitsShippedWeighted\"]\n",
    "            ]\n",
    "            .groupby([\"objective\", \"strategy\", \"mode\"])\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        # People served per item\n",
    "        temp = df_flow_no_dummy.copy()\n",
    "        temp[\"beta\"] = temp[\"impact\"].apply(lambda loc: beta[loc])\n",
    "        temp[\"peopleServed\"] = temp[\"probability\"] * temp[\"beta\"] * temp[\"flow\"]\n",
    "        people_served = (\n",
    "            temp.set_index([\"objective\", \"strategy\"])[\"peopleServed\"]\n",
    "            .groupby([\"objective\", \"strategy\"])\n",
    "            .sum()\n",
    "        )\n",
    "        people_served_per_item = pd.DataFrame(\n",
    "            people_served / (solution_stats[\"coveredDemandExcDummy\"] + 1e-7),\n",
    "            columns=[\"peopleServedPerItem\"],\n",
    "        )\n",
    "\n",
    "        # Impact of optimizing one objective on another objective\n",
    "        impact = []\n",
    "        for other in self.parameters.comparison_objectives:\n",
    "            cost = {\n",
    "                (depot.id, location.id, mode.id): value\n",
    "                for (depot, location, mode), value in costs[other].items()\n",
    "            }\n",
    "            temp = df_flow_no_dummy.copy()\n",
    "            if temp.empty:\n",
    "                raise RuntimeError(\"Empty flow matrix encountered\")\n",
    "            temp[\"cost\"] = temp.apply(\n",
    "                lambda row: cost[row[\"depot\"], row[\"location\"], row[\"mode\"]], axis=1\n",
    "            )\n",
    "            temp[\"other\"] = other\n",
    "            temp[\"impact\"] = temp[\"cost\"] * temp[\"probability\"] * temp[\"flow\"]\n",
    "            impact.append(temp.reset_index())\n",
    "        cross_impact = (\n",
    "            pd.concat(impact)\n",
    "            .groupby([\"objective\", \"strategy\", \"other\"])[[\"impact\"]]\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        return Analysis(\n",
    "            self.parameters,\n",
    "            dataset,\n",
    "            item,\n",
    "            solutions,\n",
    "            solution_stats,\n",
    "            balance_metric,\n",
    "            units_shipped,\n",
    "            people_served_per_item,\n",
    "            cross_impact,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analyzer class manages the analysis workflow. It can run analysis for a single item or for all items across various inventory scenarios. It utilizes the AnalyzerWorker to perform the computations and collects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    \"\"\"\n",
    "    Service responsible for performing optimization runs and analysis on the results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters: AnalysisParameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def run(self, dataset: Dataset, item: Item) -> Analysis:\n",
    "        worker = AnalyzerWorker(self.parameters)\n",
    "        result = worker.run(dataset, item)\n",
    "        worker.dispose()\n",
    "        return result\n",
    "\n",
    "    def run_all(self, dataset: Dataset) -> dict[tuple[str, Item], Analysis]:\n",
    "        inventory_datasets = {\n",
    "            filename: dataset.take_inventory_scenario(filename)\n",
    "            for filename in dataset.inventory_scenarios\n",
    "        }\n",
    "        tasks = [\n",
    "            (filename, inventory_dataset, item)\n",
    "            for (filename, inventory_dataset) in inventory_datasets.items()\n",
    "            for item in inventory_dataset.items\n",
    "        ]\n",
    "\n",
    "        return self._run_tasks(tasks)\n",
    "\n",
    "    def _run_tasks(self, tasks: list[tuple[str, Dataset, Item]]):\n",
    "        use_multi_processing = getenv(\"CI\", \"false\") == \"false\"\n",
    "        use_multi_processing = False\n",
    "        if use_multi_processing:\n",
    "            with Pool(\n",
    "                initializer=_analysis_worker_init, initargs=[self.parameters]\n",
    "            ) as pool:\n",
    "                result: list[tuple[str, Analysis]] = pool.map(\n",
    "                    _analysis_worker_call, tasks\n",
    "                )\n",
    "        else:\n",
    "            worker = AnalyzerWorker(self.parameters)\n",
    "            result = [\n",
    "                (filename, worker.run(dataset, item))\n",
    "                for (filename, dataset, item) in tasks\n",
    "            ]\n",
    "            worker.dispose()\n",
    "        return {\n",
    "            (filename, analysis.item): analysis\n",
    "            for (filename, analysis) in result\n",
    "            if analysis is not None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two functiomns function initializes a global worker for use in multiprocessing scenarios and calls the run method of the global worker with the provided arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analysis_worker_init(parameters):\n",
    "    global worker\n",
    "    worker = AnalyzerWorker(parameters)\n",
    "\n",
    "\n",
    "def _analysis_worker_call(arg: tuple[Dataset, Item]) -> tuple[str, Analysis]:\n",
    "    global worker\n",
    "    (filename, dataset, item) = arg\n",
    "    return (filename, worker.run(dataset, item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geting it ready for Optimization\n",
    "\n",
    "\n",
    "Let's take a look at thow the data is formated to be used in our solver. First we will set the context of our problemn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver\n",
    "The loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the Prevalent Use of Dictionaries vs Lists?\n",
    "Dictionaries are often favored over lists in production code for several reasons:\n",
    "\n",
    "-\tKey-Value Pairs: Dictionaries provide a convenient way to store and retrieve data by keys rather than by index. This makes dictionaries especially useful for scenarios where data is logically organized by unique identifiers (e.g., JSON-like structures, configurations).\n",
    "-\tReadability and Maintainability: With dictionaries, code becomes more readable and self-explanatory. For example, user['name'] is more descriptive than user[0].\n",
    "-\tFlexibility in Data Manipulation: Unlike lists, dictionaries allow quick updates and deletions by key, improving flexibility when modifying data.\n",
    "-\tPerformance: For lookups, inserts, and deletions, dictionaries (hash maps) often provide average O(1) time complexity, while lists require O(n) for searching or removing elements unless the position is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB, tupledict\n",
    "\n",
    "from src.data import Depot, Disaster, DisasterImpact, Location, TransportMode\n",
    "\n",
    "\n",
    "class AllocationStrategy(IntEnum):\n",
    "    '''\n",
    "    The AllocationStrategy enumeration defines different strategies for allocating inventory in \n",
    "    our optimization model\n",
    "\n",
    "    MinimizeTwoStage: Allows inventory reallocation in both stages of the model.\n",
    "\n",
    "    MinimizeFixedInventory: Keeps the inventory fixed as in the initial state.\n",
    "\n",
    "    WorstDepot: Allocates all inventory to the worst-performing depot to test the model's robustness.\n",
    "    '''\n",
    "    MinimizeTwoStage = 0\n",
    "    MinimizeFixedInventory = 1\n",
    "    WorstDepot = 2\n",
    "\n",
    "'''\n",
    "The SolverParameters class holds configuration settings for the solver.\n",
    "'''\n",
    "@dataclass(frozen=True)\n",
    "class SolverParameters:\n",
    "    \"\"\"\n",
    "    Parameters that influence how the solver transforms the Problem into a mathematical model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    allocation_strategy\n",
    "        If and how inventory can be reallocated\n",
    "    scale_demand\n",
    "        Whether demand should be scaled (down) to not exceed supply\n",
    "    \"\"\"\n",
    "\n",
    "    allocation_strategy: AllocationStrategy = (\n",
    "        AllocationStrategy.MinimizeFixedInventory,\n",
    "    )\n",
    "    scale_demand: bool = False\n",
    "\n",
    "'''\n",
    "We define a type alias CostMatrix for readability. It represents a dictionary mapping a \n",
    "tuple of (source location, target location, transport mode) to a cost value (e.g., transportation \n",
    "cost, time, or distance). This matrix is essential for the optimization model to calculate \n",
    "costs associated with moving goods from depots to disaster impact locations.\n",
    "'''\n",
    "CostMatrix = dict[Tuple[Location, Location, TransportMode], float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Problem:\n",
    "    '''\n",
    "    The Problem class encapsulates all the data required to define an optimization problem:\n",
    "\n",
    "    depots: A list of depots where inventory is stored.\n",
    "    \n",
    "    inventory: A dictionary mapping each depot to the quantity of inventory it holds.\n",
    "    \n",
    "    demand: A dictionary mapping each DisasterImpact location to its demand.\n",
    "    \n",
    "    disasters: A list of disasters being considered.\n",
    "    \n",
    "    probabilities: A dictionary mapping each disaster to its probability of occurrence.\n",
    "    \n",
    "    transport_modes: A list of available transport modes.\n",
    "    \n",
    "    cost: The CostMatrix containing cost information for transporting goods.*\n",
    "    '''\n",
    "    depots: list[Depot]\n",
    "    inventory: dict[Depot, int]\n",
    "    demand: dict[DisasterImpact, float]\n",
    "    disasters: list[Disaster]\n",
    "    probabilities: dict[Disaster, float]\n",
    "    transport_modes: list[TransportMode]\n",
    "    cost: CostMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Solution class stores the results obtained after solving a Problem with specific SolverParameters. It includes various metrics and variables that describe the performance and decisions made by the optimization model, such as total costs, demand coverage, dual variables, and optimal flows. The _dummy_depot is a special depot used internally to handle unmet demand in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Solution:\n",
    "    \"\"\"\n",
    "    Result of solving a single Problem with specific SolverParameters\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    total_cost_inc_dummy\n",
    "        Total transportation cost including artificial cost for using the dummy node (myObj)\n",
    "    total_cost_exc_dummy\n",
    "        Total transportation cost from the real depots (myObjNoDum)\n",
    "    total_demand\n",
    "        Total demand in the input data (myWeightedDemand)\n",
    "    covered_demand_exc_dummy\n",
    "        Demand served from real depots, averaged over all scenarios (myWeightedDemandMetNoDum)\n",
    "    fraction_of_disasters_using_dummy\n",
    "        Fraction of disaster scenarios for which not enough real inventory is available (myFractionOfDisastersUsingDummy)\n",
    "    duals_inventory_exc_dummy_plus_dummy_cost\n",
    "        Adjusted dual variables for the inventory constraints (values are independent of dummy costs) (dualsInvNoDum_PlusDummyCost)\n",
    "    duals_inventory_exc_dummy_unadjusted\n",
    "        Original dual variables for the inventory constraints, aggregated over disaster scenarios (dualsInvNoDum_UnAdj)\n",
    "    duals_inventory_exc_dummy_all\n",
    "        All original dual variables for the inventory constraints (dualsInvNoDum_All)\n",
    "    flow_exc_dummy\n",
    "        Allocation of depot inventory to disaster locations in each scenario, excluding the dummy depot (myFlowNoDum)\n",
    "    flow\n",
    "        Allocation of depot inventory to disaster locations in each scenario (myFlow)\n",
    "    optimal_inventory\n",
    "        Optimal or fixed allocation of inventory to depots (myOptInvNoDum)\n",
    "    dual_total_inventory\n",
    "        Dual variable for the total inventory constraint (dualTotInv)\n",
    "    \"\"\"\n",
    "\n",
    "    total_cost_inc_dummy: float\n",
    "    total_cost_exc_dummy: float\n",
    "    total_demand: float\n",
    "    covered_demand_exc_dummy: float\n",
    "    fraction_of_disasters_using_dummy: float\n",
    "    duals_inventory_exc_dummy_plus_dummy_cost: dict[Depot, float]\n",
    "    duals_inventory_exc_dummy_unadjusted: dict[Depot, float]\n",
    "    duals_inventory_exc_dummy_all: dict[Tuple[Disaster, Depot], float]\n",
    "    flow_exc_dummy: dict[Tuple[Disaster, Depot, DisasterImpact, TransportMode], float]\n",
    "    flow: dict[Tuple[Disaster, Depot, DisasterImpact, TransportMode], float]\n",
    "    optimal_inventory: dict[Depot, float]\n",
    "    dual_total_inventory: float\n",
    "\n",
    "    _dummy_depot: Depot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StochasticSolver class encapsulates the logic for solving stochastic optimization problems. The class variables _threshold_cost_elim and _threshold_cost_dummy are large constants used to effectively eliminate certain arcs or penalize the use of the dummy depot in the optimization model. The constructor initializes a dummy location and creates a Gurobi environment with specific parameters (e.g., disabling output and setting single-threaded execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticSolver:\n",
    "    _threshold_cost_elim: float = 1e9\n",
    "    _threshold_cost_dummy: float = 1e9\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dummy = Location(\"DUMMY\", \"\", \"\", 0, 0)\n",
    "        self._env = gp.Env(params={\"OutputFlag\": 0, \"Threads\": 1})\n",
    "    '''\n",
    "    The dispose method releases resources associated with the Gurobi environment. \n",
    "    It's important to call this method after solving problems to prevent resource \n",
    "    leaks, especially when solving multiple problems in a loop.\n",
    "    '''\n",
    "    def dispose(self):\n",
    "        self._env.dispose()\n",
    "        self._env = None\n",
    "\n",
    "    '''\n",
    "    In the solve method, we begin by preparing the data for the optimization model:\n",
    "    \n",
    "    We include both real depots and the dummy depot in our list of sources.\n",
    "    \n",
    "    Demand scaling is performed if specified in the parameters.\n",
    "    \n",
    "    We construct the list of possible arcs in the transportation network, filtering out those with \n",
    "    prohibitive costs unless they involve the dummy depot.\n",
    "    \n",
    "    We calculate the cost for each arc, adjusting for disaster probabilities to reflect the expected\n",
    "    cost across scenarios.\n",
    "    \n",
    "    We initialize a Gurobi model named \"StochLP\" for solving the stochastic linear program\n",
    "    '''\n",
    "\n",
    "    def solve(self, problem: Problem, parameters: SolverParameters) -> Solution:\n",
    "        sources = problem.depots + [self._dummy]\n",
    "\n",
    "        demand = (\n",
    "            self._scale_demand(problem) if parameters.scale_demand else problem.demand\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        The arcs represent the possible transportation routes from the supply sources (depots and a dummy depot)\n",
    "        to the disaster-impacted locations for each disaster scenario and transport mode. Specifically, \n",
    "        each arc is a tuple (k, i, j, v) where:\n",
    "\n",
    "        k i a disaster scenario from the list of disasters being considered.\n",
    "        \n",
    "        i is a source location, which can be a real depot or a special dummy depot used to model unmet demand.\n",
    "        \n",
    "        j is a disaster impact location, representing an area affected by disaster k where relief goods are needed.\n",
    "        \n",
    "        v is a transport mode, such as road, air, or sea transport.\n",
    "        These arcs form the edges of the transportation network in the model. They define all feasible routes along \n",
    "        which relief items can be transported from depots to impacted locations under different disaster scenarios and transportation options.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        arcs = gp.tuplelist(\n",
    "            [\n",
    "                (k, i, j, v)\n",
    "                for i in sources\n",
    "                for k in problem.disasters\n",
    "                for j in k.impacted_locations\n",
    "                for v in problem.transport_modes\n",
    "                if (\n",
    "                    self._get_arc_cost(problem.cost, i, j, v)\n",
    "                    < self._threshold_cost_elim\n",
    "                )\n",
    "                or (i == self._dummy)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        arc_cost = {\n",
    "            (k, i, j, v): self._get_arc_cost(problem.cost, i, j, v)\n",
    "            * problem.probabilities[k]\n",
    "            for (k, i, j, v) in arcs\n",
    "        }\n",
    "\n",
    "        model = gp.Model(\"StochLP\", env=self._env)\n",
    "\n",
    "        # First stage variable: Quantity to be allocated to each depot\n",
    "        x: tupledict[Depot, gp.Var] = model.addVars(problem.depots, lb=0, name=\"x\")\n",
    "\n",
    "        # Second stage variable: Quantity transported from (real or dummy) depot to disaster locations using each mode of transport\n",
    "        y: tupledict[\n",
    "            Tuple[Disaster, Union[Depot, Location], DisasterImpact, TransportMode],\n",
    "            gp.Var,\n",
    "        ] = model.addVars(arcs, lb=0, obj=arc_cost, name=\"y\")\n",
    "\n",
    "        # Constraint: Total incoming arc flow must cover demand for each disaster location\n",
    "        model.addConstrs(\n",
    "            (\n",
    "                y.sum(k, \"*\", j, \"*\") == demand[j]\n",
    "                for k in problem.disasters\n",
    "                for j in k.impacted_locations\n",
    "            ),\n",
    "            name=\"satisfyDemand\",\n",
    "        )\n",
    "\n",
    "        # Constraint: Total outgoing arc flow must match initial or reallocated inventory\n",
    "        inventory_balance: tupledict[\n",
    "            Tuple[Disaster, Depot], gp.Constr\n",
    "        ] = model.addConstrs(\n",
    "            (\n",
    "                y.sum(k, i, \"*\", \"*\") <= x[i]\n",
    "                for k in problem.disasters\n",
    "                for i in problem.depots\n",
    "            ),\n",
    "            name=\"satisfySupply\",\n",
    "        )\n",
    "\n",
    "        # Constraint: Ensure inventory reallocation matches total existing inventory\n",
    "        total_initial_inventory = sum(problem.inventory.values())\n",
    "        match_total_inventory = model.addConstr(x.sum() == total_initial_inventory)\n",
    "\n",
    "        '''\n",
    "        We define a helper function fix_inventory_balance to fix the inventory variables (x) to specific values, \n",
    "        effectively turning them into constants in the model. Based on the allocation_strategy\n",
    "        \n",
    "        MinimizeFixedInventory: We fix the inventory variables to their initial values, preventing reallocation.\n",
    "        \n",
    "        WorstDepot: We iterate over all depots, allocating all inventory to each one individually to find the depot \n",
    "        that results in the worst (highest) objective value. We then fix the inventory allocation to that depot, \n",
    "        which is useful for stress-testing the model's performance under adverse conditions.\n",
    "        '''\n",
    "        def fix_inventory_balance(values: dict[Disaster, float]):\n",
    "            for key, value in values.items():\n",
    "                x[key].LB = x[key].UB = value\n",
    "\n",
    "        if parameters.allocation_strategy == AllocationStrategy.MinimizeFixedInventory:\n",
    "            fix_inventory_balance(problem.inventory)\n",
    "        elif parameters.allocation_strategy == AllocationStrategy.WorstDepot:\n",
    "            worst_depot = None\n",
    "            worst_objective = -1e100\n",
    "            for depot in problem.depots:\n",
    "                centralized_inventory = {\n",
    "                    other: total_initial_inventory if other == depot else 0\n",
    "                    for other in problem.depots\n",
    "                }\n",
    "                fix_inventory_balance(centralized_inventory)\n",
    "                model.optimize()\n",
    "                if model.Status != GRB.Status.OPTIMAL:\n",
    "                    raise RuntimeError(\"Could not solve model to optimality\")\n",
    "                if model.ObjVal > worst_objective:\n",
    "                    worst_depot = depot\n",
    "                    worst_objective = model.ObjVal\n",
    "            centralized_inventory = {\n",
    "                other: total_initial_inventory if other == worst_depot else 0\n",
    "                for other in problem.depots\n",
    "            }\n",
    "            fix_inventory_balance(centralized_inventory)\n",
    "\n",
    "        model.optimize()\n",
    "        if model.Status != GRB.Status.OPTIMAL:\n",
    "            raise RuntimeError(\"Could not solve model to optimality\")\n",
    "\n",
    "        # Total transport cost\n",
    "        total_cost_inc_dummy = model.ObjVal\n",
    "        dummy_cost = sum(\n",
    "            var.X * var.Obj for var in y.select(\"*\", self._dummy, \"*\", \"*\")\n",
    "        )\n",
    "        total_cost_exc_dummy = total_cost_inc_dummy - dummy_cost\n",
    "\n",
    "        # Demand met without using the dummy node\n",
    "        covered_demand_by_dummy = sum(\n",
    "            y[k, i, j, v].X * problem.probabilities[k]\n",
    "            for (k, i, j, v) in arcs.select(\"*\", self._dummy, \"*\", \"*\")\n",
    "        )\n",
    "        total_demand = sum(\n",
    "            local_demand * problem.probabilities[j.disaster]\n",
    "            for (j, local_demand) in demand.items()\n",
    "        )\n",
    "        covered_demand_exc_dummy = total_demand - covered_demand_by_dummy\n",
    "\n",
    "        # Flow in solution\n",
    "        solution_y = {key: y[key].X for key in arcs}\n",
    "\n",
    "        # Fraction of disaster scenarios in which the dummy supply is used\n",
    "        fraction_of_disasters_using_dummy = len(\n",
    "            [\n",
    "                disaster\n",
    "                for disaster in problem.disasters\n",
    "                if sum(\n",
    "                    solution_y[key]\n",
    "                    for key in arcs.select(disaster, self._dummy, \"*\", \"*\")\n",
    "                )\n",
    "                > 0\n",
    "            ]\n",
    "        ) / len(problem.disasters)\n",
    "\n",
    "        # Dual variables for the inventory balance constraints\n",
    "        dual_correction = fraction_of_disasters_using_dummy * self._threshold_cost_dummy\n",
    "        duals_inventory_exc_dummy_unadjusted = {\n",
    "            i: sum(inventory_balance[k, i].Pi for k in problem.disasters)\n",
    "            for i in problem.depots\n",
    "        }\n",
    "        duals_inventory_exc_dummy_plus_dummy_cost = {\n",
    "            i: dual_correction + pi\n",
    "            for (i, pi) in duals_inventory_exc_dummy_unadjusted.items()\n",
    "        }\n",
    "        duals_inventory_exc_dummy_all = {\n",
    "            (k, i): constr.Pi for (k, i), constr in inventory_balance.items()\n",
    "        }\n",
    "\n",
    "        flow = {(k, i, j, v): var.X for (k, i, j, v), var in y.items() if var.X > 0}\n",
    "\n",
    "        flow_exc_dummy = {\n",
    "            (k, i, j, v): value\n",
    "            for (k, i, j, v), value in flow.items()\n",
    "            if i != self._dummy\n",
    "        }\n",
    "\n",
    "        optimal_inventory = {depot: var.X for depot, var in x.items()}\n",
    "\n",
    "        dual_total_inventory = (\n",
    "            match_total_inventory.Pi\n",
    "            if parameters.allocation_strategy\n",
    "            == AllocationStrategy.MinimizeFixedInventory\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        Finally, we package all the extracted metrics and variables into a Solution object and return it. This object \n",
    "        provides a comprehensive view of the optimization results, enabling further analysis or reporting.\n",
    "        '''\n",
    "\n",
    "        return Solution(\n",
    "            total_cost_inc_dummy,\n",
    "            total_cost_exc_dummy,\n",
    "            total_demand,\n",
    "            covered_demand_exc_dummy,\n",
    "            fraction_of_disasters_using_dummy,\n",
    "            duals_inventory_exc_dummy_plus_dummy_cost,\n",
    "            duals_inventory_exc_dummy_unadjusted,\n",
    "            duals_inventory_exc_dummy_all,\n",
    "            flow_exc_dummy,\n",
    "            flow,\n",
    "            optimal_inventory,\n",
    "            dual_total_inventory,\n",
    "            self._dummy,\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    The _get_arc_cost method retrieves the cost of transporting goods from a source to a target using a specific transport mode. If the \n",
    "    source is the dummy depot, it returns a high dummy cost to penalize \n",
    "    its use. If there is no cost information available for a given arc \n",
    "    (i.e., cell is None), it returns a high elimination threshold cost \n",
    "    to effectively remove that arc from consideration.\n",
    "    '''\n",
    "\n",
    "    def _get_arc_cost(\n",
    "        self,\n",
    "        cost: CostMatrix,\n",
    "        source: Location,\n",
    "        target: DisasterImpact,\n",
    "        mode: TransportMode,\n",
    "    ):\n",
    "        if source == self._dummy:\n",
    "            return self._threshold_cost_dummy\n",
    "        cell = cost.get((source, target.location, mode))\n",
    "        return self._threshold_cost_elim if cell == None else cell\n",
    "\n",
    "    '''\n",
    "    The _scale_demand method adjusts the demand levels so that total demand \n",
    "    does not exceed total supply. For each disaster, it calculates a scaling \n",
    "    factor based on the ratio of total supply to total demand. It then applies \n",
    "    this factor to the demand at each impacted location. This is useful in scenarios \n",
    "    where supply constraints are tight, ensuring that the optimization model \n",
    "    operates within feasible limits.\n",
    "    \n",
    "    '''\n",
    "    def _scale_demand(self, problem: Problem) -> dict[DisasterImpact, float]:\n",
    "        supply = sum(problem.inventory.values())\n",
    "        result: dict[DisasterImpact, float] = {}\n",
    "        for disaster in problem.disasters:\n",
    "            total_demand = sum(\n",
    "                problem.demand[location] for location in disaster.impacted_locations\n",
    "            )\n",
    "            factor = min(1, supply / total_demand) if total_demand > 1e-6 else 1\n",
    "            for location in disaster.impacted_locations:\n",
    "                result[location] = factor * problem.demand[location]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After diving into the code, it's clear that it's exceptionally well-crafted, following best practices and maintaining high readability. The lead developer shared some key insights that contributed to this level of quality. These takeaways not only explain how the code reached its current state but also offer valuable lessons for future projects. Let's explore these insights to understand what makes the code stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Balancing Commenting on Different Projects with Different People Involved**\n",
    "\n",
    "\tWhen working on different projects involving different teams, it is crucial to adapt your commenting style to meet the team’s specific needs and coding standards. Here are some tips for balancing commenting styles across projects:\n",
    "\n",
    "\t- Understand Team Preferences: Some teams prefer more verbose comments to ensure all members understand the code, while others might lean towards minimal commenting to keep code clean. Spend time understanding each team’s culture and coding standards.\n",
    "\n",
    "\t\n",
    "\t- Project-specific Documentation: Tailor the level and style of commenting to fit the project’s needs. For example, a data science team might require detailed explanations of algorithms and mathematical steps, while a software engineering team may focus more on architectural and functional documentation.\n",
    "\t-\tReusable Templates: Create reusable templates or guidelines for comments that can be adapted easily for each project. For instance, standardize on docstring formats (e.g., Google-style, reStructuredText) and comment structures that can be tweaked as per the project’s requirements.\n",
    "\t-\tConsistency is Key: Regardless of the project, maintain consistency in your commenting style within a single codebase. This reduces cognitive load and makes it easier for team members to understand the code quickly.\n",
    "\t-\tCommunicate Clearly: Have an open discussion with your teammates about their preferences. Clearly defined comment guidelines should be set and documented at the beginning of each project.\n",
    "\n",
    "2. **Why the Prevalent Use of Dictionaries vs Lists?**\n",
    "\tIn the provided code examples, dictionaries are extensively used over lists, and this choice offers several advantages:\n",
    "\n",
    "\t-\t**Key-Value Access**: Dictionaries allow for fast retrieval of data using keys, which is essential when dealing with entities identified by unique attributes. For example, the Dataset class uses dictionaries like inventory, probabilities, and distance to map complex relationships between depots, disasters, and locations. \n",
    "\t\t```python\n",
    "\t\tinventory: dict[Tuple[Depot, Item], int]\n",
    "\t\tDistanceMatrix: Dict[Tuple[Location, Location, TransportMode], DistanceInfo]\n",
    "\t\t```\n",
    "\n",
    "\t-   **Readability**: Using dictionaries makes the code more self-explanatory. Accessing ```inventory[(depot, item)]``` is clearer than using index-based access in a list, improving code readability.\n",
    "\n",
    "\t-\t**Performance**: Dictionaries provide average O(1) time complexity for lookups, which is beneficial when working with large datasets, as is common in disaster logistics modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Testing and Debugging**\n",
    "\n",
    "\tTesting and debugging are crucial for ensuring the reliability of production code, especially in complex systems like our stochastic solver where very real world impacts can occur if bugs go un-noticed. We haven't shown them specifically here for brevity, however, let's take a look at how we change our code to accomidate them:\n",
    "\n",
    "\t-\t**Error Handling**: In the StochasticSolver class, we include checks after optimization to ensure that the model has reached an optimal solution. If not, we raise a RuntimeError with a clear message. For example:\n",
    "\n",
    "\t\t```python\n",
    "\t\tmodel.optimize()\n",
    "\t\tif model.Status != GRB.Status.OPTIMAL:\n",
    "\t\t\traise RuntimeError(\"Could not solve model to optimality\")\n",
    "\t\t```\n",
    "\t-\t**Modularity for Testing**: The code is organized into modular functions and methods, such as _get_arc_cost and _scale_demand, which makes unit testing more manageable. Each function can be tested independently to verify that it behaves as expected.\n",
    "\n",
    "\t\t-\t**Unit Tests**: Write unit tests for every function or module to ensure that each part works as expected independently. Unit tests should be automated and run frequently to catch errors early.\n",
    "\t\t-\t**Integration Tests**: Test interactions between different modules to catch integration-related issues. Use mocking or stubbing to isolate dependencies when needed.\n",
    "\t\n",
    "\t\t-\t**Logging**: Make effective use of logging with different levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). Logging provides a way to understand what the code was doing at the time an error occurred without having to run it in a debugging mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Any Particular Naming Rules? Discuss with Others**\n",
    "\n",
    "\tConsistent and meaningful naming conventions are crucial for collaboration:\n",
    "\n",
    "\t-\t**CamelCase vs. snake_case**: Choose a naming convention that fits the projectand stick with it throughout the codebase. In this case we have:\n",
    "\t\t- CamelCase for classes like ```DisasterImpact```, ```AnalyzerWorker```, and ```StochasticSolver```\n",
    "\t\t-\tsnake_case for variables and functions like ```total_cost_inc_dummy```, ```_filter_dataset```, and ```optimal_inventory``` ) \n",
    "\n",
    "\t-\t**Descriptive Names**: Use descriptive names that clearly indicate the purpose of a variable, function, or class. Avoid abbreviations unless they are standard and well-known.\n",
    "\t\n",
    "\t-\t**Prefixed Naming**: When needed, prefix variables to indicate their type or usage (is_ for booleans, num_ for numbers, str_ for strings). This is more common in languages with weaker type systems.\n",
    "\n",
    "\t\n",
    "\t-\t**Team Agreements**: Have a discussion with your team to establish a naming convention that everyone agrees on. Document these conventions in a style guide that is accessible to all team members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. **Error Handling**\n",
    "\n",
    "\tProper error handling is vital to ensure robustness in production code:\n",
    "\t-\t**Specific Exceptions**: The code raises specific exceptions with informative messages. For example, in the ```take_inventory_scenario``` method of the Dataset class, a RuntimeError is raised if an inventory scenario is not found\n",
    "\t\t```python\n",
    "\t\tif filename not in self.inventory_scenarios:\n",
    "\t\traise RuntimeError(\"Inventory scenario not found\")\n",
    "\t\t```\n",
    "\t-\t**Fallback Mechanisms**: In the solver, when a solution is not optimal, the code doesn't proceed blindly but instead stops execution, which prevents cascading errors and makes debugging easier.\n",
    "\t-\t**Validation Checks**: Before performing operations, the code often checks for potential issues, such as verifying that the total inventory is not zero before proceeding with the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. **Global Variables**\n",
    "\n",
    "\tGlobal variables should generally be avoided in production code due to potential issues with maintainability and concurrency:\n",
    "\t-\t**Encapsulation**:Encapsulate variables within classes or functions to limit their scope and prevent unintended modifications. Classes like ```AnalyzerWorker``` and ```StochasticSolver``` encapsulate their data and methods, preventing external interference.\n",
    "\t-\t**Controlled Scope**: In multiprocessing scenarios, global variables are used judiciously. For instance, a global worker is initialized in the ```_analysis_worker_init``` function for use in worker processes, but its scope is limited and managed carefully.\n",
    "\t\t```python\n",
    "\t\tdef _analysis_worker_init(parameters):\n",
    "\t\t\tglobal worker\n",
    "\t\t\tworker = AnalyzerWorker(parameters)\n",
    "\n",
    "\t\t```\n",
    "\t-\t**Unintended Side Effects**: Global variables can lead to unexpected side effects, making the code harder to debug and maintain.\n",
    "\t-\t**Thread Safety**: In multi-threaded applications, global variables can lead to race conditions if not handled properly.\n",
    "\t-\t**Use Constants Judiciously**: If you must use global variables, restrict them to constants (immutable values) and use them sparingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "7. **Typing**\n",
    "\n",
    "\tStrong typing adds robustness and clarity to production code:\n",
    "\n",
    "\t-\t**Function Annotations**: Methods specify the types of their arguments and return values, which aids in understanding what kinds of data are expected.\n",
    "\t\t```python\n",
    "\t\tdef _get_cost_element(\n",
    "\t\t\tself, cell: DistanceInfo, objective: SolverObjective, item: Item):\n",
    "\t\t```\n",
    "\n",
    "\n",
    "\t-\t**Type Aliases**: Defining type aliases like CostMatrix and DistanceMatrix makes complex types more readable and maintainable.\n",
    "\t\t```python\n",
    "\t\tCostMatrix = dict[Tuple[Location, Location, TransportMode], float]\n",
    "\t\t```\n",
    "\t-\t**Static Type Checking**: Using type hints allows tools like mypy to perform static type checking, catching potential type errors before runtime.\n",
    "\n",
    "\n",
    "\n",
    "By following these principles, you can improve code quality, collaboration, and maintainability in a production environment, while also tailoring your practices to different projects and teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runing it All\n",
    "\n",
    "So what would runing all this look like? Lets take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = \"vanuatu_simple\"\n",
    "\n",
    "# Run optimization\n",
    "reader = CsvProblemReader()\n",
    "dataset = reader.read(DATA_DIR / COUNTRY)\n",
    "\n",
    "parameters = AnalysisParameters()\n",
    "analyzer = Analyzer(parameters)\n",
    "result = analyzer.run_all(dataset)\n",
    "\n",
    "exisiting_stock_df = calc_exisiting_stock_df(dataset, country=COUNTRY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_DIR = Path.cwd().resolve()\n",
    "DATA_DIR = WORKSPACE_DIR / \"data\"\n",
    "TEST_DATA_DIR = WORKSPACE_DIR / \"data\" / \"test_data\"\n",
    "DASHBOARD_OUTPUT_PATH = WORKSPACE_DIR / \"dashboard_output\"\n",
    "\n",
    "\n",
    "priority_change_df = create_priority_change(result, country=COUNTRY)\n",
    "\n",
    "all_duals_df = pd.DataFrame()\n",
    "for key, value in result.items():\n",
    "    scenario = get_scenario(key)\n",
    "\n",
    "    if scenario != \"actual\":\n",
    "        continue\n",
    "\n",
    "    duals_df = calc_duals_by_warehouse(value)\n",
    "    all_duals_df = pd.concat([all_duals_df, duals_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics():\n",
    "    ### ProvinceAssessDF dashboard file\n",
    "    provinces_df = pd.read_csv(DATA_DIR / COUNTRY / \"province_lookup.csv\")\n",
    "    provinces_df: pd.DataFrame = ProvinceLookupDF.validate(provinces_df)\n",
    "    province_assess_df: pd.DataFrame = create_province_assess_df(\n",
    "        all_duals_df,\n",
    "        provinces_df,\n",
    "        COUNTRY,\n",
    "    )\n",
    "\n",
    "    ### WhStockAssessDF dashboard file\n",
    "    duals_df_copy = all_duals_df.copy()\n",
    "    wh_stock_assess: pd.DataFrame = item_stock_assess(duals_df_copy, provinces_df, COUNTRY)\n",
    "\n",
    "\n",
    "    ### Reallocation dashboard file\n",
    "    reallocation_df, single_warehouse_df = reallocation_dashboard_files(\n",
    "        wh_stock_assess.copy(),\n",
    "        result,\n",
    "        COUNTRY,\n",
    "        wh_stock_assess=wh_stock_assess,\n",
    "    )\n",
    "\n",
    "    ### Disaster totals dashboard file\n",
    "    dis_totals_df = create_disaster_totals(dataset, COUNTRY)\n",
    "\n",
    "\n",
    "    ### Save dashboard files\n",
    "\n",
    "    if not (DASHBOARD_OUTPUT_PATH / COUNTRY).exists():\n",
    "        (DASHBOARD_OUTPUT_PATH / COUNTRY).mkdir(parents=True)\n",
    "\n",
    "    exisiting_stock_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"exisiting_stock.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    priority_change_df = priority_change_df.loc[\n",
    "        priority_change_df[BalMetricsDashboard.run_pct] == \"actual\"\n",
    "    ]\n",
    "    priority_change_df = priority_change_df.drop(columns=[BalMetricsDashboard.run_pct])\n",
    "    priority_change_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"priority_change.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    province_assess_df = province_assess_df.drop(columns=[ProvinceAssessDF.time])\n",
    "    province_assess_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"province_assess.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    dec_wh_stock_assess = wh_stock_assess.drop(columns=[ItemProvinceAssessDF.time_hms])\n",
    "    dec_wh_stock_assess.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"wh_stock_assess_as_decimal.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    wh_stock_assess = wh_stock_assess.drop(columns=[ItemProvinceAssessDF.time_hms])\n",
    "    wh_stock_assess.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"wh_stock_assess.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    reallocation_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"reallocation.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    single_warehouse_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"single_warehouse.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    dis_totals_df.to_csv(\n",
    "        DASHBOARD_OUTPUT_PATH / COUNTRY / \"disaster_totals.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping It Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude this journey through the evolution of ESUPS and the STOCKHOLM platform, it's inspiring to reflect on how a collaborative spirit and a shared vision can drive meaningful change. ESUPS began as a modest initiative, a simple spreadsheet aimed at cataloging disaster relief supplies to prevent the inefficiencies witnessed during the Nepal earthquake response. However, it was through strategic partnerships with academia, non-profit organizations, and industry leaders that ESUPS transformed into a global force reshaping disaster preparedness and response.\n",
    "\n",
    "The collaboration with universities like Penn State and MIT infused ESUPS with cutting-edge research and fresh perspectives. Students and academics brought innovative optimization models and data analytics techniques to the table, enhancing the platform's ability to make data-driven decisions. These partnerships not only advanced ESUPS's mission but also provided invaluable real-world experience for students eager to apply their skills to pressing global challenges.\n",
    "\n",
    "Non-profit organizations and NGOs contributed on-the-ground insights and a deep understanding of humanitarian needs. Their involvement ensured that the solutions developed were practical, culturally sensitive, and aligned with the realities of disaster-stricken areas. This synergy between theoretical models and practical application exemplifies how diverse stakeholders can come together to tackle complex problems effectively.\n",
    "\n",
    "For students and aspiring professionals, the story of ESUPS serves as a powerful reminder that impactful ideas often start small but can grow exponentially through collaboration and determination. It highlights the potential each individual has to contribute to a better world, especially when leveraging skills in data science, optimization, and technology.\n",
    "\n",
    "You, too, can implement your own ideas for a better world. Whether it's developing innovative algorithms to optimize resource allocation, creating platforms that enhance communication among aid organizations, or initiating community projects that address local issues, your contributions matter. The tools and techniques explored in this notebook are not just academic exercises—they are gateways to real-world applications that can save lives and improve outcomes in critical situations.\n",
    "\n",
    "We encourage you to take the lessons learned from ESUPS and apply them in your own pursuits:\n",
    "\n",
    "-   Embrace Collaboration: Seek partnerships with universities, non-profits, and industry experts. Collaboration amplifies impact.\n",
    "-   Leverage Technology for Good: Use your technical skills to develop solutions that address societal challenges.\n",
    "-   Think Globally, Act Locally: Start with problems you are passionate about in your community; small changes can lead to significant impact.\n",
    "-   Stay Curious and Innovative: Continuously explore new ideas and approaches. Innovation thrives on curiosity and the willingness to challenge the status quo.\n",
    "By following in the footsteps of ESUPS and other pioneering initiatives, you can be part of a generation that not only understands the complexities of global challenges but actively contributes to solving them. The future of disaster response, humanitarian aid, and many other fields depends on innovative thinkers and dedicated doers like you.\n",
    "\n",
    "Remember, significant change often begins with a single idea and grows through collaboration, perseverance, and a shared commitment to making the world a better place. We look forward to seeing how you will apply these insights to create your own impactful narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Happened to the Second Derivative?\n",
    "\n",
    "In the realm of optimization, the second derivative of a function, often referred to as the Hessian matrix in multi-dimensional spaces, provides valuable insights into the curvature of the objective function. Specifically, it helps us understand how the slope of the function changes, enabling us to distinguish between local maxima, minima, and saddle points. This information is crucial for methods like Newton's method, which uses the second derivative to make precise adjustments toward the optimal solution.\n",
    "\n",
    "However, in complex, real-world problems such as disaster relief logistics, relying on the second derivative presents significant challenges:\n",
    "\n",
    "1. **Non-Linear and Non-Smooth Functions**: The objective functions in disaster relief are often non-linear and may have discontinuities due to the various constraints and sudden changes in variables (e.g., supply chain disruptions, varying demand). Calculating the second derivative for such functions can be impractical or impossible, as these irregularities disrupt the smooth curvature that the second derivative relies on.\n",
    "\n",
    "2. **High Dimensionality**: The optimization problems in disaster relief involve numerous variables, such as quantities of different supplies, multiple transportation routes, and varying time constraints. The Hessian matrix, representing the second derivatives with respect to all pairs of variables, becomes exceedingly large and computationally expensive to compute and store. In high-dimensional spaces, the complexity and cost of computing the Hessian can outweigh its benefits.\n",
    "\n",
    "3. **Dynamic and Stochastic Environments**: The environment in which disaster relief operations take place is dynamic and often stochastic, with elements of uncertainty and unpredictability. These factors introduce variability that is difficult to capture with static second-order information. As the situation evolves, the second derivative may no longer accurately represent the current state of the system.\n",
    "\n",
    "4. **Local Optima**: In complex optimization landscapes, there are often multiple local optima. The second derivative provides local information and might lead optimization algorithms to converge to these local optima rather than the global optimum. This is particularly problematic in disaster relief, where finding the best possible solution can make a critical difference.\n",
    "\n",
    "5. **Lack of Closed-Form Solutions**: When dealing with high-power polynomials and other complex functions, closed-form solutions for finding the roots do not always exist. The second derivative may help approximate solutions in simpler cases, but as the degree of the polynomial increases, finding exact solutions becomes intractable. This is particularly relevant in disaster logistics, where the relationships between variables can be modeled by high-degree polynomials without straightforward solutions.\n",
    "\n",
    "Given these challenges, alternative optimization methods are preferred. Gradient-based approaches like gradient descent rely solely on the first derivative (gradient) and can handle larger, non-linear, and non-smooth problems more effectively. Additionally, heuristic and metaheuristic methods, such as genetic algorithms, simulated annealing, and particle swarm optimization, do not rely on derivative information at all. These methods explore the solution space more broadly and can escape local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Largrangian\n",
    "\n",
    "The Lagrangian is a fundamental concept in optimization, especially when dealing with constraints. In simple terms, the Lagrangian function incorporates both the original objective function and the constraints of the problem, allowing us to solve constrained optimization problems more effectively. For disaster relief optimization, the Lagrangian helps in balancing the trade-off between minimizing costs and meeting the demand for supplies under various constraints, such as limited resources or maximum allowable transit times. By introducing Lagrange multipliers, we transform the problem into an unconstrained one, making it easier to find optimal solutions that respect all necessary conditions.\n",
    "\n",
    "The Lagrangian approach is particularly powerful when second-order methods are impractical due to the reasons mentioned above. By converting a constrained problem into an unconstrained one, it simplifies the complexity and allows for the use of gradient-based methods or other optimization techniques that do not rely on second-order derivatives. This flexibility makes the Lagrangian method a crucial tool in the optimization toolbox, especially for complex, real-world problems like disaster relief logistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Branch and Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Branch and Bound** method is a powerful optimization technique used for solving **integer programming** problems where some or all decision variables must take on integer values. The fundamental idea is to systematically explore branches of possible solutions while “bounding” areas that cannot contain the optimal solution, thereby narrowing down the search efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 185,
     "metadata": {
      "image/png": {
       "height": 200,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='images/branch-and-bound.png',width=400, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-Step Process:**\n",
    "1.\t**Relaxation of Constraints:**\n",
    "To start, we relax the integer constraints, treating all variables as continuous. This relaxation converts the integer programming problem into a standard linear programming problem, which we can solve efficiently using the **Simplex Method**. The result of this relaxed problem provides a baseline solution that may not necessarily satisfy the integer requirements. For example, if the optimal solution of the relaxed problem gives us a variable  x = 2.6 , we know this is not a valid solution if  x  must be an integer.\n",
    "\n",
    "2.\t**Creating Subproblems by Branching:**\n",
    "When a solution contains a variable that is not an integer (like  x = 2.6 ), we create two subproblems by “branching” on this variable:\n",
    "    -\t**Branch 1:** Add a constraint that rounds  x  **down** to the nearest integer,  $x \\leq 2$.\n",
    "    -\t**Branch 2:** Add a constraint that rounds  x  **up** to the next integer,  $x \\geq 3$.\n",
    "These constraints split the solution space into two smaller, more manageable subproblems. By dividing the problem this way, we explore different regions of the solution space that could lead to an optimal integer solution.\n",
    "\n",
    "3.\t**Solving and Bounding:**\n",
    "We then solve each of these subproblems, again using the Simplex Method, to find the best possible solutions within the new constraints. For each subproblem:\n",
    "    -\tIf a subproblem yields a solution that is **feasible** (i.e., all variables are integers) and **better** than the current best-known solution, it becomes the new candidate for the optimal solution.\n",
    "    -\tIf a subproblem’s solution is not feasible (i.e., it still has non-integer variables) or if it cannot improve upon the current best solution, we bound it off. This means we stop exploring that branch further, as it cannot possibly lead to a better solution.\n",
    "\n",
    "4.\t**Recursive Exploration:**\n",
    "The process continues recursively: for each subproblem that still contains non-integer solutions, we branch again, creating further subproblems with tighter constraints. This systematic exploration and bounding help eliminate large swaths of the solution space that cannot contain the optimal solution, making the search much more efficient.\n",
    "\n",
    "5.\t**Convergence to the Optimal Solution:**\n",
    "The Branch and Bound algorithm terminates when all subproblems have been explored or bounded off. At this point, the best feasible solution found is guaranteed to be the **optimal solution** for the original integer programming problem. Unlike heuristic methods, this approach provides a definitive, mathematically-proven optimal solution.\n",
    "\n",
    "By combining the efficiency of the Simplex Method for solving linear problems with a systematic search for feasible integer solutions, Branch and Bound is a robust method for tackling complex optimization problems where some or all decision variables must be integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the context of disaster relief logistics, the environment is not just dynamic but also inherently stochastic. This means that many of the variables involved—such as demand for supplies, transportation times, or the availability of resources—are not deterministic. Instead, they are influenced by a range of unpredictable factors, from weather conditions to sudden changes in infrastructure availability or the emergence of new areas needing assistance. Stochastic systems introduce randomness and uncertainty into optimization problems, requiring specialized approaches that can handle variability effectively.\n",
    "\n",
    "1.\tStochastic Optimization: Unlike deterministic optimization methods that assume a fixed environment, stochastic optimization incorporates randomness directly into the problem formulation. Techniques such as Stochastic Gradient Descent (SGD) adapt the optimization process to the presence of noise by using randomly selected subsets of data to update solutions iteratively. This allows the method to converge more quickly on large datasets or complex environments typical of disaster relief operations, where real-time data is continuously being updated.\n",
    "2.\tModeling Uncertainty with Probabilistic Constraints: Disaster relief problems often require the modeling of uncertainty in the constraints themselves. For instance, a probabilistic constraint might state that the likelihood of delivering sufficient supplies to a location must be at least 95%. Methods like Chance-Constrained Programming and Robust Optimization are designed to handle such constraints, ensuring that solutions are viable even under uncertain conditions.\n",
    "3.\tMonte Carlo Methods and Simulation-Based Approaches: For highly uncertain systems where analytical solutions are challenging to derive, Monte Carlo simulations provide a practical alternative. These simulations model different possible scenarios by generating random samples from probability distributions associated with uncertain parameters. By running a large number of such simulations, decision-makers can estimate the expected outcomes and identify strategies that are robust across a range of possible futures. This is particularly valuable in disaster relief logistics, where preparing for the worst-case scenarios can save lives.\n",
    "4.\tReinforcement Learning (RL) for Adaptive Decision Making: RL techniques are increasingly applied to stochastic optimization problems in logistics and supply chain management. In the RL framework, an agent learns to make decisions by interacting with an environment, receiving feedback in the form of rewards or penalties. This approach allows the agent to develop strategies that perform well under uncertainty and adapt to evolving conditions, making it highly suitable for disaster relief scenarios where rapid and flexible response is essential.\n",
    "5.\tMarkov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs): When dealing with sequential decision-making under uncertainty, MDPs provide a powerful framework to model the problem. In MDPs, decisions are made in stages, and each decision affects both immediate rewards and future states of the system. POMDPs extend this framework to situations where the system’s state is not fully observable, which is often the case in disaster response. For example, the exact condition of a road or the need level in a community may only be partially known due to limited communication. These models help optimize policies that account for both known and unknown elements, providing a balanced approach to decision-making under uncertainty.\n",
    "6.\tDynamic Programming and Approximate Dynamic Programming: In problems characterized by stochasticity and high dimensionality, exact solutions are often impractical. Dynamic Programming (DP) breaks down the problem into smaller, more manageable subproblems, solving them recursively. However, when the state space is too large (a common issue in real-world logistics), Approximate Dynamic Programming (ADP) or reinforcement learning-based approaches like Q-learning or policy gradients are used to find near-optimal solutions without exhaustive computation.\n",
    "\n",
    "In conclusion, handling stochastic systems requires a blend of probabilistic modeling, simulation, and adaptive learning techniques. For disaster relief logistics, where uncertainty is a given and stakes are high, these methods provide the necessary tools to devise strategies that are not only optimal in theory but also robust and flexible in practice. The ability to incorporate randomness, adapt to evolving scenarios, and provide solutions under uncertainty makes stochastic systems analysis a cornerstone of modern optimization approaches in complex, real-world environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurobi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}